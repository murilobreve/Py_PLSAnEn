{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48cc8056-a783-4f4e-87c4-87bfd869af34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import sqrt\n",
    "import xarray as xr\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0338a854-85fb-4d3a-a00b-d456466fe5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "time_interval_reconstruction = [\"2018-01-01 00:00:00\", \"2019-12-31 23:54:00\"]\n",
    "predict_variable = 'WSPDchyv2_2011_2019'\n",
    "k = 3\n",
    "folder_path = '/Users/Murilo/weather_data'\n",
    "netCDF_resolution = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b52db45d-b74f-44bf-a0c1-729a6323f6d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chyv2_2011_2019.nc':         WSPD   WDIR  GST  ATMP                time\n",
       " 0        2.3  210.0  3.6   8.3 2011-01-01 00:00:00\n",
       " 1        2.4  197.0  3.6   8.3 2011-01-01 00:06:00\n",
       " 2        2.7  190.0  3.7   8.1 2011-01-01 00:12:00\n",
       " 3        2.6  186.0  3.5   8.0 2011-01-01 00:18:00\n",
       " 4        2.6  190.0  3.9   7.8 2011-01-01 00:24:00\n",
       " ...      ...    ...  ...   ...                 ...\n",
       " 788875   2.9  229.0  4.6  11.1 2019-12-31 23:30:00\n",
       " 788876   2.9  225.0  4.2  11.0 2019-12-31 23:36:00\n",
       " 788877   3.0  224.0  4.8  11.0 2019-12-31 23:42:00\n",
       " 788878   2.9  226.0  3.9  10.9 2019-12-31 23:48:00\n",
       " 788879   3.1  229.0  4.3  10.9 2019-12-31 23:54:00\n",
       " \n",
       " [788880 rows x 5 columns],\n",
       " 'cryv2_2011_2019.nc':         WSPD   WDIR  GST         PRES                time\n",
       " 0        2.4  171.0  2.5  1024.000000 2011-01-01 00:00:00\n",
       " 1        2.5  169.0  2.6  1024.099976 2011-01-01 00:06:00\n",
       " 2        2.4  159.0  2.7  1024.000000 2011-01-01 00:12:00\n",
       " 3        3.1  147.0  3.3  1024.099976 2011-01-01 00:18:00\n",
       " 4        3.2  153.0  3.3  1024.099976 2011-01-01 00:24:00\n",
       " ...      ...    ...  ...          ...                 ...\n",
       " 788875   2.6  248.0  3.9  1007.500000 2019-12-31 23:30:00\n",
       " 788876   2.4  240.0  3.8  1007.599976 2019-12-31 23:36:00\n",
       " 788877   2.3  243.0  3.7  1007.599976 2019-12-31 23:42:00\n",
       " 788878   2.4  243.0  4.0  1007.599976 2019-12-31 23:48:00\n",
       " 788879   1.9  237.0  2.8  1007.700012 2019-12-31 23:54:00\n",
       " \n",
       " [788880 rows x 5 columns],\n",
       " 'domv2_2011_2019.nc':         WSPD   WDIR  GST         PRES  ATMP                time\n",
       " 0        1.6  187.0  1.8  1023.799988   8.6 2011-01-01 00:00:00\n",
       " 1        1.4  200.0  1.7  1024.000000   8.3 2011-01-01 00:06:00\n",
       " 2        1.4  198.0  1.7  1023.900024   8.1 2011-01-01 00:12:00\n",
       " 3        1.5  188.0  1.7  1024.000000   7.9 2011-01-01 00:18:00\n",
       " 4        1.1  179.0  1.2  1024.000000   7.9 2011-01-01 00:24:00\n",
       " ...      ...    ...  ...          ...   ...                 ...\n",
       " 788875   5.4  248.0  6.7  1007.799988  11.3 2019-12-31 23:30:00\n",
       " 788876   5.2  251.0  6.2  1007.900024  11.1 2019-12-31 23:36:00\n",
       " 788877   4.7  251.0  5.7  1007.900024  11.0 2019-12-31 23:42:00\n",
       " 788878   4.9  252.0  5.6  1007.900024  11.0 2019-12-31 23:48:00\n",
       " 788879   4.3  254.0  5.4  1007.900024  10.9 2019-12-31 23:54:00\n",
       " \n",
       " [788880 rows x 6 columns],\n",
       " 'kptv2_2011_2019.nc':         WSPD   WDIR  GST  WTMP                time\n",
       " 0        3.6  185.0  3.7   2.8 2011-01-01 00:00:00\n",
       " 1        3.1  171.0  3.6   2.8 2011-01-01 00:06:00\n",
       " 2        3.1  176.0  3.4   2.8 2011-01-01 00:12:00\n",
       " 3        3.8  184.0  4.1   2.8 2011-01-01 00:18:00\n",
       " 4        3.3  181.0  3.8   2.8 2011-01-01 00:24:00\n",
       " ...      ...    ...  ...   ...                 ...\n",
       " 788875   4.1  256.0  4.8   9.4 2019-12-31 23:30:00\n",
       " 788876   3.8  252.0  5.0   9.4 2019-12-31 23:36:00\n",
       " 788877   3.7  250.0  4.7   9.4 2019-12-31 23:42:00\n",
       " 788878   4.3  245.0  5.3   9.4 2019-12-31 23:48:00\n",
       " 788879   4.6  243.0  5.5   9.4 2019-12-31 23:54:00\n",
       " \n",
       " [788880 rows x 5 columns],\n",
       " 'mnpv2_2011_2019.nc':         WSPD   WDIR  GST         PRES  ATMP  WTMP                time\n",
       " 0        1.7  203.0  2.5  1024.400024   8.4   6.0 2011-01-01 00:00:00\n",
       " 1        1.8  203.0  2.5  1024.400024   8.5   6.0 2011-01-01 00:06:00\n",
       " 2        1.8  203.0  2.5  1024.400024   8.5   6.1 2011-01-01 00:12:00\n",
       " 3        2.1  205.0  3.0  1024.500000   8.4   6.1 2011-01-01 00:18:00\n",
       " 4        2.3  207.0  3.0  1024.500000   8.4   6.1 2011-01-01 00:24:00\n",
       " ...      ...    ...  ...          ...   ...   ...                 ...\n",
       " 788875   2.1  239.0  2.8  1007.500000  11.3  10.2 2019-12-31 23:30:00\n",
       " 788876   1.5  243.0  2.5  1007.599976  11.1  10.3 2019-12-31 23:36:00\n",
       " 788877   1.6  232.0  2.3  1007.599976  11.0  10.2 2019-12-31 23:42:00\n",
       " 788878   1.5  240.0  2.1  1007.700012  11.0  10.3 2019-12-31 23:48:00\n",
       " 788879   1.5  242.0  2.0  1007.700012  10.8  10.3 2019-12-31 23:54:00\n",
       " \n",
       " [788880 rows x 7 columns],\n",
       " 'swpv2_2011_2019.nc':                PRES  WTMP                time\n",
       " 0       1024.300049   2.1 2011-01-01 00:00:00\n",
       " 1       1024.300049   2.1 2011-01-01 00:06:00\n",
       " 2       1024.300049   2.1 2011-01-01 00:12:00\n",
       " 3       1024.300049   2.1 2011-01-01 00:18:00\n",
       " 4       1024.300049   2.1 2011-01-01 00:24:00\n",
       " ...             ...   ...                 ...\n",
       " 788875  1007.400024   9.8 2019-12-31 23:30:00\n",
       " 788876  1007.500000   9.8 2019-12-31 23:36:00\n",
       " 788877  1007.400024   9.8 2019-12-31 23:42:00\n",
       " 788878  1007.500000   9.8 2019-12-31 23:48:00\n",
       " 788879  1007.500000   9.8 2019-12-31 23:54:00\n",
       " \n",
       " [788880 rows x 3 columns],\n",
       " 'wdsv2_2011_2019.nc':         WSPD   WDIR  GST         PRES  ATMP                time\n",
       " 0        3.8  200.0  4.1  1023.299988   NaN 2011-01-01 00:00:00\n",
       " 1        3.5  194.0  3.7  1023.400024   NaN 2011-01-01 00:06:00\n",
       " 2        3.3  189.0  3.6  1023.500000   NaN 2011-01-01 00:12:00\n",
       " 3        3.0  184.0  3.2  1023.500000   NaN 2011-01-01 00:18:00\n",
       " 4        2.9  178.0  3.2  1023.500000   NaN 2011-01-01 00:24:00\n",
       " ...      ...    ...  ...          ...   ...                 ...\n",
       " 788875   7.2  228.0  8.4  1006.900024  11.4 2019-12-31 23:30:00\n",
       " 788876   6.7  229.0  7.5  1007.000000  11.2 2019-12-31 23:36:00\n",
       " 788877   7.0  235.0  7.6  1006.900024  11.0 2019-12-31 23:42:00\n",
       " 788878   6.9  236.0  7.3  1007.000000  11.0 2019-12-31 23:48:00\n",
       " 788879   7.1  233.0  8.0  1007.000000  11.0 2019-12-31 23:54:00\n",
       " \n",
       " [788880 rows x 6 columns],\n",
       " 'ykrv2_2011_2019.nc':         WSPD   WDIR  GST         PRES  ATMP                time\n",
       " 0        3.0  221.0  3.2  1023.500000   9.5 2011-01-01 00:00:00\n",
       " 1        3.5  224.0  4.0  1023.500000   9.5 2011-01-01 00:06:00\n",
       " 2        3.5  222.0  3.7  1023.599976   9.4 2011-01-01 00:12:00\n",
       " 3        3.4  216.0  3.6  1023.599976   8.9 2011-01-01 00:18:00\n",
       " 4        3.4  208.0  3.6  1023.500000   9.0 2011-01-01 00:24:00\n",
       " ...      ...    ...  ...          ...   ...                 ...\n",
       " 788875   6.0  237.0  6.4  1007.700012  10.5 2019-12-31 23:30:00\n",
       " 788876   6.0  235.0  6.9  1007.700012  10.5 2019-12-31 23:36:00\n",
       " 788877   5.6  235.0  6.2  1007.700012  10.5 2019-12-31 23:42:00\n",
       " 788878   5.4  235.0  5.8  1007.900024  10.6 2019-12-31 23:48:00\n",
       " 788879   5.1  235.0  5.5  1007.900024  10.4 2019-12-31 23:54:00\n",
       " \n",
       " [788880 rows x 6 columns],\n",
       " 'yktv2_2011_2019.nc':         WSPD   WDIR  GST         PRES  ATMP  WTMP                time\n",
       " 0        0.7  211.0  1.0  1023.500000   8.8   2.4 2011-01-01 00:00:00\n",
       " 1        1.1  220.0  1.3  1023.599976   8.1   2.4 2011-01-01 00:06:00\n",
       " 2        0.7  212.0  0.8  1023.599976   6.4   2.4 2011-01-01 00:12:00\n",
       " 3        1.0  209.0  1.4  1023.500000   7.4   2.5 2011-01-01 00:18:00\n",
       " 4        1.3  201.0  1.4  1023.599976   7.6   2.5 2011-01-01 00:24:00\n",
       " ...      ...    ...  ...          ...   ...   ...                 ...\n",
       " 788875   3.2  224.0  4.1  1006.799988  10.7   8.8 2019-12-31 23:30:00\n",
       " 788876   2.9  227.0  3.9  1006.900024  10.7   8.8 2019-12-31 23:36:00\n",
       " 788877   3.1  230.0  4.2  1006.900024  10.7   8.9 2019-12-31 23:42:00\n",
       " 788878   3.4  228.0  4.7  1006.900024  10.6   8.9 2019-12-31 23:48:00\n",
       " 788879   3.9  225.0  5.0  1006.799988  10.6   8.9 2019-12-31 23:54:00\n",
       " \n",
       " [788880 rows x 7 columns]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_dataframes = {}  # This dictionary will store DataFrames for each nc file\n",
    "\n",
    "nc_files = [f for f in os.listdir(folder_path) if f.endswith('.nc')]\n",
    "\n",
    "for nc_file in nc_files:\n",
    "    with xr.open_dataset(os.path.join(folder_path, nc_file)) as ds:\n",
    "        data_dict = {}\n",
    "        \n",
    "        # Dynamically load all variables from the file\n",
    "        for var_name, variable in ds.data_vars.items():\n",
    "            data = variable.values\n",
    "            \n",
    "            data = np.where(data == 9.969210e+36, np.nan, data)\n",
    "            fraction_available = np.sum(~np.isnan(data)) / data.size\n",
    "\n",
    "            # If more than 80% of the data is available, store it in the dictionary\n",
    "            if fraction_available > 0.80:\n",
    "                data_dict[var_name] = data\n",
    "\n",
    "        # Convert the time variable\n",
    "        if 'time' in ds.coords:\n",
    "            time_data = ds['time']   \n",
    "            \n",
    "            # Add time data to the dictionary\n",
    "            data_dict['time'] = time_data\n",
    "        \n",
    "        # Convert the data dictionary to a DataFrame\n",
    "        df = pd.DataFrame(data_dict)\n",
    "        \n",
    "        # Store this DataFrame in our all_dataframes dictionary\n",
    "        all_dataframes[nc_file] = df\n",
    "\n",
    "all_dataframes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "464e91dc-192a-43d2-854c-f55d35c03ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   WSPDchyv2_2011_2019  WDIRchyv2_2011_2019  GSTchyv2_2011_2019  \\\n",
      "0                  2.3                210.0                 3.6   \n",
      "1                  2.4                197.0                 3.6   \n",
      "2                  2.7                190.0                 3.7   \n",
      "3                  2.6                186.0                 3.5   \n",
      "4                  2.6                190.0                 3.9   \n",
      "\n",
      "   ATMPchyv2_2011_2019                time  WSPDcryv2_2011_2019  \\\n",
      "0                  8.3 2011-01-01 00:00:00                  2.4   \n",
      "1                  8.3 2011-01-01 00:06:00                  2.5   \n",
      "2                  8.1 2011-01-01 00:12:00                  2.4   \n",
      "3                  8.0 2011-01-01 00:18:00                  3.1   \n",
      "4                  7.8 2011-01-01 00:24:00                  3.2   \n",
      "\n",
      "   WDIRcryv2_2011_2019  GSTcryv2_2011_2019  PREScryv2_2011_2019  \\\n",
      "0                171.0                 2.5          1024.000000   \n",
      "1                169.0                 2.6          1024.099976   \n",
      "2                159.0                 2.7          1024.000000   \n",
      "3                147.0                 3.3          1024.099976   \n",
      "4                153.0                 3.3          1024.099976   \n",
      "\n",
      "   WSPDdomv2_2011_2019  ...  WDIRykrv2_2011_2019  GSTykrv2_2011_2019  \\\n",
      "0                  1.6  ...                221.0                 3.2   \n",
      "1                  1.4  ...                224.0                 4.0   \n",
      "2                  1.4  ...                222.0                 3.7   \n",
      "3                  1.5  ...                216.0                 3.6   \n",
      "4                  1.1  ...                208.0                 3.6   \n",
      "\n",
      "   PRESykrv2_2011_2019  ATMPykrv2_2011_2019  WSPDyktv2_2011_2019  \\\n",
      "0          1023.500000                  9.5                  0.7   \n",
      "1          1023.500000                  9.5                  1.1   \n",
      "2          1023.599976                  9.4                  0.7   \n",
      "3          1023.599976                  8.9                  1.0   \n",
      "4          1023.500000                  9.0                  1.3   \n",
      "\n",
      "   WDIRyktv2_2011_2019  GSTyktv2_2011_2019  PRESyktv2_2011_2019  \\\n",
      "0                211.0                 1.0          1023.500000   \n",
      "1                220.0                 1.3          1023.599976   \n",
      "2                212.0                 0.8          1023.599976   \n",
      "3                209.0                 1.4          1023.500000   \n",
      "4                201.0                 1.4          1023.599976   \n",
      "\n",
      "   ATMPyktv2_2011_2019  WTMPyktv2_2011_2019  \n",
      "0                  8.8                  2.4  \n",
      "1                  8.1                  2.4  \n",
      "2                  6.4                  2.4  \n",
      "3                  7.4                  2.5  \n",
      "4                  7.6                  2.5  \n",
      "\n",
      "[5 rows x 42 columns]\n",
      "Start Time: 2011-01-01 00:00:00\n",
      "End Time: 2019-12-31 23:54:00\n",
      "✅ SUCCESS ✅\n",
      "The time interval provided is INSIDE the range of the available data.\n"
     ]
    }
   ],
   "source": [
    "merged_df = None\n",
    "\n",
    "for filename, df in all_dataframes.items():\n",
    "    # Skip the 'time' column for renaming\n",
    "    rename_dict = {col: col + filename[:-3] if col != 'time' else col for col in df.columns}\n",
    "    df = df.rename(columns=rename_dict)\n",
    "\n",
    "    if merged_df is None:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        # Merge the DataFrame with the main merged_df on 'time'\n",
    "        merged_df = pd.merge(merged_df, df, on='time', how='outer')\n",
    "\n",
    "print(merged_df.head())\n",
    "\n",
    "start_time = merged_df['time'].iloc[0]\n",
    "end_time = merged_df['time'].iloc[-1]\n",
    "\n",
    "print(f\"Start Time: {start_time}\")\n",
    "print(f\"End Time: {end_time}\")\n",
    "\n",
    "# Extract client times\n",
    "client_start_time = pd.Timestamp(time_interval_reconstruction[0])\n",
    "client_end_time = pd.Timestamp(time_interval_reconstruction[1])\n",
    "\n",
    "# Check if client times fall within the dataframe's time range\n",
    "if client_start_time < start_time or client_end_time > end_time:\n",
    "    print(\"⚠️ WARNING ⚠️\")\n",
    "    print(\"The time interval provided is OUTSIDE the range of the available data.\")\n",
    "    print(f\"Available data range: {start_time} to {end_time}\")\n",
    "    print(f\"Interval: {client_start_time} to {client_end_time}\")\n",
    "    print(\"Please provide a valid time range.\")\n",
    "else:\n",
    "    print(\"✅ SUCCESS ✅\")\n",
    "    print(\"The time interval provided is INSIDE the range of the available data.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "468c0501-bc2c-4abc-be4f-fb1d3afd4f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and testing based on the client's time range\n",
    "training_data = merged_df[(merged_df['time'] < client_start_time) | (merged_df['time'] > client_end_time)]\n",
    "testing_data = merged_df[(merged_df['time'] >= client_start_time) & (merged_df['time'] <= client_end_time)]\n",
    "\n",
    "# Make a copy of the original dataframes for later use\n",
    "training_with_na = training_data.copy()\n",
    "testing_with_na = testing_data.copy()\n",
    "\n",
    "# Drop NaNs for training and testing datasets\n",
    "training_na = training_data.dropna()\n",
    "testing_na = testing_data.dropna()\n",
    "\n",
    "# Define X_train and Y_train\n",
    "X_train = training_na.drop(columns=['time', predict_variable])\n",
    "Y_train = training_na[predict_variable]\n",
    "\n",
    "# Define X_val and Y_val\n",
    "X_val = testing_na.drop(columns=['time', predict_variable])\n",
    "Y_val = testing_na[predict_variable]\n",
    "merged_df.to_csv('merged_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff8d3a00-a8e2-4773-bf61-114be90bee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      time         0         1\n",
      "0      2011-01-01 00:00:00       NaN       NaN\n",
      "1      2011-01-01 00:06:00       NaN       NaN\n",
      "2      2011-01-01 00:12:00       NaN       NaN\n",
      "3      2011-01-01 00:18:00       NaN       NaN\n",
      "4      2011-01-01 00:24:00       NaN       NaN\n",
      "...                    ...       ...       ...\n",
      "788875 2019-12-31 23:30:00 -0.350146 -3.791242\n",
      "788876 2019-12-31 23:36:00 -0.692525 -3.770110\n",
      "788877 2019-12-31 23:42:00 -0.741699 -3.608891\n",
      "788878 2019-12-31 23:48:00 -0.738279 -3.726141\n",
      "788879 2019-12-31 23:54:00 -0.777975 -3.658096\n",
      "\n",
      "[788880 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# 1. Combine training_with_na and testing_with_na to get full dataset\n",
    "full_data_with_na = pd.concat([training_with_na, testing_with_na], axis=0)\n",
    "\n",
    "# Define the PLS model\n",
    "pls = PLSRegression(n_components=2)\n",
    "pls.fit(X_train, Y_train)\n",
    "\n",
    "# Fit the PLS model on training data\n",
    "X_all = pd.concat([X_train, X_val], axis=0)\n",
    "\n",
    "# 2. Transform the data\n",
    "X_all_transformed = pls.transform(X_all)\n",
    "\n",
    "# Convert transformed data to DataFrame for easier manipulation\n",
    "X_all_transformed_df = pd.DataFrame(X_all_transformed, index=X_all.index)\n",
    "\n",
    "# Merge with the original timestamps\n",
    "result = pd.merge(full_data_with_na[['time']], X_all_transformed_df, left_index=True, right_index=True, how='left')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a432f48-5898-45ff-a770-4fe83b725a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0         1         2         3         4         5         6   \\\n",
      "0            NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "1            NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "2            NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "3            NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "4            NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
      "...          ...       ...       ...       ...       ...       ...       ...   \n",
      "788875  -0.40107       NaN       NaN -0.350146 -0.692525 -0.741699 -0.738279   \n",
      "788876       NaN       NaN -0.350146 -0.692525 -0.741699 -0.738279 -0.777975   \n",
      "788877       NaN -0.350146 -0.692525 -0.741699 -0.738279 -0.777975       NaN   \n",
      "788878 -0.350146 -0.692525 -0.741699 -0.738279 -0.777975       NaN       NaN   \n",
      "788879 -0.692525 -0.741699 -0.738279 -0.777975       NaN       NaN       NaN   \n",
      "\n",
      "              7         8         9         10        11        12        13  \n",
      "0            NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "1            NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "2            NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "3            NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "4            NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
      "...          ...       ...       ...       ...       ...       ...       ...  \n",
      "788875 -3.561303       NaN       NaN -3.791242  -3.77011 -3.608891 -3.726141  \n",
      "788876       NaN       NaN -3.791242  -3.77011 -3.608891 -3.726141 -3.658096  \n",
      "788877       NaN -3.791242  -3.77011 -3.608891 -3.726141 -3.658096       NaN  \n",
      "788878 -3.791242  -3.77011 -3.608891 -3.726141 -3.658096       NaN       NaN  \n",
      "788879  -3.77011 -3.608891 -3.726141 -3.658096       NaN       NaN       NaN  \n",
      "\n",
      "[788880 rows x 14 columns]\n"
     ]
    }
   ],
   "source": [
    "# Assuming the variables k and result are already defined\n",
    "nb_LVs = result.shape[1]\n",
    "\n",
    "fit_data = np.full((k, nb_LVs), np.nan)\n",
    "\n",
    "LVs = np.vstack([fit_data, result, fit_data])\n",
    "\n",
    "LVs_df = pd.DataFrame(LVs)\n",
    "LVs_df.index = LVs_df[0]\n",
    "\n",
    "# Using LVs_df instead of LVs\n",
    "Y = []\n",
    "for j in range(1, LVs_df.shape[1]):\n",
    "    data_into_vectors = []\n",
    "    for i in range(2 * k + 1):\n",
    "        data_into_vectors.append(LVs_df.iloc[i:i + len(LVs_df) - 2*k, j].values)       \n",
    "    Y.append(np.column_stack(data_into_vectors))\n",
    "\n",
    "Y_all = np.column_stack(Y)\n",
    "Y_all_df = pd.DataFrame(Y_all)\n",
    "\n",
    "print(Y_all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9045152b-839f-4d2b-ba69-2f550f07d586",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert merged_df.shape[0] == result.shape[0], \"Dataframes have different sizes!\"\n",
    "\n",
    "# Use a coluna 'time' de 'merged_df' para criar um array booleano\n",
    "time_conditions = (merged_df['time'] < client_start_time) | (merged_df['time'] > client_end_time)\n",
    "\n",
    "Y_analogues = Y_all_df[time_conditions]\n",
    "Y_pred = Y_all_df[~time_conditions]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3083d161-d4d7-4d4c-a5d4-cbd6667cba89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                       | 0/175200 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                                                           | 16/175200 [00:04<12:20:49,  3.94it/s]\u001b[A\n",
      "  0%|                                                                           | 24/175200 [00:12<28:02:39,  1.74it/s]\u001b[A\n",
      "  0%|                                                                           | 32/175200 [00:20<36:38:59,  1.33it/s]\u001b[A\n",
      "  0%|                                                                           | 40/175200 [00:28<41:27:02,  1.17it/s]\u001b[A\n",
      "  0%|                                                                           | 48/175200 [00:37<44:20:32,  1.10it/s]\u001b[A\n",
      "  0%|                                                                           | 56/175200 [00:45<46:02:08,  1.06it/s]\u001b[A\n",
      "  0%|                                                                           | 64/175200 [00:53<47:30:48,  1.02it/s]\u001b[A\n",
      "  0%|                                                                           | 72/175200 [01:01<48:12:34,  1.01it/s]\u001b[A\n",
      "  0%|                                                                           | 80/175200 [01:10<49:14:22,  1.01s/it]\u001b[A\n",
      "  0%|                                                                           | 88/175200 [01:18<49:14:33,  1.01s/it]\u001b[A\n",
      "  0%|                                                                           | 96/175200 [01:27<50:49:08,  1.04s/it]\u001b[A\n",
      "  0%|                                                                          | 104/175200 [01:36<51:49:25,  1.07s/it]\u001b[A\n",
      "  0%|                                                                          | 112/175200 [01:45<53:01:57,  1.09s/it]\u001b[A\n",
      "  0%|                                                                          | 120/175200 [01:54<53:03:37,  1.09s/it]\u001b[A\n",
      "  0%|                                                                          | 128/175200 [02:02<52:58:50,  1.09s/it]\u001b[A\n",
      "  0%|                                                                          | 136/175200 [02:11<53:01:20,  1.09s/it]\u001b[A\n",
      "  0%|                                                                          | 144/175200 [02:20<53:02:34,  1.09s/it]\u001b[A\n",
      "  0%|                                                                          | 152/175200 [02:29<53:15:00,  1.10s/it]\u001b[A\n",
      "  0%|                                                                          | 160/175200 [02:37<52:45:02,  1.08s/it]\u001b[A\n",
      "  0%|                                                                          | 168/175200 [02:46<52:44:11,  1.08s/it]\u001b[A\n",
      "  0%|                                                                          | 176/175200 [02:55<52:47:05,  1.09s/it]\u001b[A\n",
      "  0%|                                                                          | 184/175200 [03:03<52:47:11,  1.09s/it]\u001b[A\n",
      "  0%|                                                                          | 192/175200 [03:12<52:39:07,  1.08s/it]\u001b[A\n",
      "  0%|                                                                          | 200/175200 [03:21<52:43:27,  1.08s/it]\u001b[A\n",
      "  0%|                                                                          | 208/175200 [03:29<52:26:48,  1.08s/it]\u001b[A\n",
      "  0%|                                                                          | 216/175200 [03:38<53:12:40,  1.09s/it]\u001b[A\n",
      "  0%|                                                                          | 224/175200 [03:47<52:48:42,  1.09s/it]\u001b[A\n",
      "  0%|                                                                          | 232/175200 [03:55<52:34:08,  1.08s/it]\u001b[A\n",
      "  0%|                                                                          | 240/175200 [04:04<52:09:26,  1.07s/it]\u001b[A\n",
      "  0%|                                                                          | 248/175200 [04:12<52:01:53,  1.07s/it]\u001b[A\n",
      "  0%|                                                                          | 256/175200 [04:21<52:34:05,  1.08s/it]\u001b[A\n",
      "  0%|                                                                          | 264/175200 [04:30<52:41:13,  1.08s/it]\u001b[A\n",
      "  0%|                                                                          | 272/175200 [04:38<52:34:27,  1.08s/it]\u001b[A\n",
      "  0%|                                                                          | 280/175200 [04:47<52:13:13,  1.07s/it]\u001b[A\n",
      "  0%|                                                                          | 288/175200 [04:55<51:37:45,  1.06s/it]\u001b[A\n",
      "  0%|▏                                                                         | 296/175200 [05:03<51:07:45,  1.05s/it]\u001b[A\n",
      "  0%|▏                                                                         | 304/175200 [05:12<51:04:45,  1.05s/it]\u001b[A\n",
      "  0%|▏                                                                         | 312/175200 [05:20<50:52:28,  1.05s/it]\u001b[A\n",
      "  0%|▏                                                                         | 320/175200 [05:29<51:28:02,  1.06s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "# Convert dataframes to numpy arrays for faster computation\n",
    "Y_analogues_np = Y_analogues.values\n",
    "Y_pred_np = Y_pred.values\n",
    "\n",
    "# Function to compute the most similar row for a single row in Y_pred\n",
    "def find_most_similar(row):\n",
    "    distances = np.nansum((Y_analogues_np - row) ** 2, axis=1)\n",
    "    return np.argmin(distances)\n",
    "\n",
    "# Use joblib to parallelize the operation\n",
    "n_jobs = 8  # This will use all CPU cores. Adjust as needed.\n",
    "similar_rows = Parallel(n_jobs=n_jobs)(delayed(find_most_similar)(row) for row in tqdm(Y_pred_np))\n",
    "\n",
    "Y_pred['most_similar_row_in_Y_analogues'] = similar_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22224ea1-2a3a-445a-b374-7dac75051922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframes to PyTorch tensors and move to GPU\n",
    "Y_analogues_tensor = torch.tensor(Y_analogues.values, dtype=torch.float32).cuda()\n",
    "Y_pred_tensor = torch.tensor(Y_pred.values, dtype=torch.float32).cuda()\n",
    "\n",
    "# Function to compute the most similar row for a single row in Y_pred using GPU\n",
    "def find_most_similar_gpu(row):\n",
    "    distances = torch.sum((Y_analogues_tensor - row) ** 2, axis=1)\n",
    "    _, min_index = torch.min(distances, 0)\n",
    "    return min_index.item()\n",
    "\n",
    "# Compute most similar rows using GPU\n",
    "similar_rows = [find_most_similar_gpu(row) for row in tqdm(Y_pred_tensor, desc=\"Finding Similar Rows\")]\n",
    "\n",
    "Y_pred['most_similar_row_in_Y_analogues'] = similar_rows\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b7d33e-93a6-4fef-adf3-23450ca3935e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
